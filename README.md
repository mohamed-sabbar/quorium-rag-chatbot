# RAG Q&A Chatbot ğŸ¤–

A Retrieval-Augmented Generation (RAG) application that answers questions based on provided documents. Built with FastAPI (backend), Next.js (frontend), LangChain, and Docker.

---

## ğŸ“‹ Table of Contents

1. [Features](#-features)
2. [Architecture](#-architecture)
3. [Requirements](#-requirements)
4. [Installation](#-installation)
5. [Usage](#-usage)
6. [Project Structure](#-project-structure)
7. [API Documentation](#-api-documentation)
8. [Docker Commands](#-docker-commands)
9. [Configuration](#-configuration)
10. [Testing](#-testing)
11. [Troubleshooting](#-troubleshooting)
12. [Performance](#-performance)
13. [Technologies](#-technologies)
14. [Security](#-security)
15. [Future Improvements](#-future-improvements)

---

## ğŸš€ Features

- âœ… **Document Ingestion**: Support for PDF, TXT, and Markdown files
- âœ… **Complete RAG Pipeline**: Chunking, embeddings, vector search
- âœ… **FastAPI Backend**: RESTful API with automatic documentation
- âœ… **Next.js Frontend**: Modern chat interface with React 19 and TypeScript
- âœ… **Docker Ready**: Complete orchestration with docker-compose
- âœ… **Source Citations**: Displays source documents for answers
- âœ… **GROQ LLM Integration**: GROQ API integration for fast inference
- âœ… **Persistent Vector Store**: Chroma for embedding storage

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   RAG Q&A APPLICATION                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Frontend (Next.js)  â”‚      â”‚  Backend (FastAPI)   â”‚  â”‚
â”‚  â”‚  Port 3000           â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Port 8000           â”‚  â”‚
â”‚  â”‚                      â”‚      â”‚                      â”‚  â”‚
â”‚  â”‚  - Chat Interface    â”‚      â”‚  - /ask endpoint     â”‚  â”‚
â”‚  â”‚  - React 19          â”‚      â”‚  - /health check     â”‚  â”‚
â”‚  â”‚  - Tailwind CSS      â”‚      â”‚  - API docs          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                         â–²                  â”‚
â”‚                                         â”‚                  â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚                    â–¼                              â–¼        â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚        â”‚  LangChain RAG      â”‚      â”‚  Chroma Vector   â”‚  â”‚
â”‚        â”‚  Pipeline           â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Store           â”‚  â”‚
â”‚        â”‚                     â”‚      â”‚  (Persistent)    â”‚  â”‚
â”‚        â”‚  - Document Loader  â”‚      â”‚                  â”‚  â”‚
â”‚        â”‚  - Text Splitter    â”‚      â”‚  - Embeddings    â”‚  â”‚
â”‚        â”‚  - Query/Retrieval  â”‚      â”‚  - Similarity    â”‚  â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚    Search        â”‚  â”‚
â”‚                â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                â–¼                                           â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚        â”‚  Sentence Transformers (MiniLM)     â”‚           â”‚
â”‚        â”‚  Model: all-MiniLM-L6-v2            â”‚           â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Requirements

Before starting, ensure you have:

- **Docker Desktop** installed and running
- **4GB+ RAM** available
- **Internet connection** (for downloading models)
- **GROQ API Key** (free at https://console.groq.com/keys)

---

## ğŸ“¦ Installation

### Step 1: Clone the Repository

```bash
git clone <your-repo-url>
cd quorium-rag-chatbot
```

### Step 2: Configure GROQ API Key

The GROQ API key is already embedded in the `docker-compose.yml` file:

```yaml
environment:
  - GROQ_API_KEY=${GROQ_API_KEY:-}
```

> âš ï¸ **Important**: Replace this key with your own from https://console.groq.com/keys

To change the key, edit `docker-compose.yml`:

```yaml
services:
  backend:
    environment:
      - GROQ_API_KEY=your_new_api_key_here
```

### Step 3: Prepare Documents

Place your documents in the `backend/data/docs/` folder:

```bash
# Create the folder if it doesn't exist
mkdir -p ./backend/data/docs

# Copy your documents
cp /path/to/your/documents/* ./backend/data/docs/
```

**Supported formats**:

- ğŸ“„ PDF
- ğŸ“ TXT
- ğŸ“‹ Markdown (.md)

### Step 4: Build Docker Images

Make the script executable and build the images:

```bash
# Make script executable (Linux/Mac/WSL)
chmod +x docker.sh

# Build Docker images
./docker.sh build
```

This will:

- Build the backend Python container
- Build the frontend Node.js container
- Download all dependencies
- **Time**: 3-5 minutes (first time)

### Step 5: Start Services

```bash
# Start services in the background
./docker.sh up
```

Services will be available at:

- **Frontend**: http://localhost:3000
- **Backend API**: http://localhost:8000
- **API Documentation**: http://localhost:8000/docs

### Step 6: Ingest Documents

Open a **new terminal** and run:

```bash
# Ingest documents
./docker.sh ingest
```

You will see:

```
ğŸ“„ Loading documents...
Loaded 10 raw documents

âœ‚ï¸ Splitting documents...
Generated 250 chunks

ğŸ§  Creating embeddings and saving vector store...

âœ… Ingestion complete!
```

**Estimated time**: 30-90 seconds depending on document size

### Step 7: Use the Application

1. Open your browser: **http://localhost:3000**
2. Type your question in the chat interface
3. Get answers based on your documents with sources

---

## ğŸ’» Usage

### Via Web Interface

1. Go to http://localhost:3000
2. Enter your question
3. Get a response based on the ingested documents
4. View the source documents used

### Via REST API

#### Ask a Question

```bash
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What is this document about?"}'
```

**Response**:

```json
{
  "answer": "Based on the provided documents, this document...",
  "sources": ["document1.pdf", "document2.txt"]
}
```

#### Health Check

```bash
curl http://localhost:8000/health
```

**Response**:

```json
{
  "status": "ok",
  "vector_store_initialized": true
}
```

#### Interactive API Documentation

Visit: http://localhost:8000/docs

You will see the Swagger UI documentation with all endpoints

---

## ğŸ“ Project Structure

```
quorium-rag-chatbot/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“„ docker-compose.yml           # Docker orchestration
â”œâ”€â”€ ğŸ“„ docker.ps1                   # PowerShell script (optional)
â”œâ”€â”€ ğŸ“„ docker.sh                    # Bash script
â”‚
â”œâ”€â”€ ğŸ“ backend/
â”‚   â”œâ”€â”€ ğŸ“„ main.py                  # FastAPI application
â”‚   â”œâ”€â”€ ğŸ“„ rag_pipeline.py          # RAG pipeline logic
â”‚   â”œâ”€â”€ ğŸ“„ ingest.py                # Document ingestion script
â”‚   â”œâ”€â”€ ğŸ“„ requirements.txt         # Python dependencies
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile               # Backend container
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ data/
â”‚   â”‚   â””â”€â”€ ğŸ“ docs/                # Your documents (PDF, TXT, MD)
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ vector_store/            # Chroma vector database
â”‚       â”œâ”€â”€ chroma.sqlite3
â”‚       â””â”€â”€ [collections]/
â”‚
â”œâ”€â”€ ğŸ“ frontend/
â”‚   â”œâ”€â”€ ğŸ“„ package.json             # Node.js dependencies
â”‚   â”œâ”€â”€ ğŸ“„ tsconfig.json            # TypeScript configuration
â”‚   â”œâ”€â”€ ğŸ“„ next.config.ts           # Next.js configuration
â”‚   â”œâ”€â”€ ğŸ“„ tailwind.config.js       # Tailwind configuration
â”‚   â”œâ”€â”€ ğŸ“„ Dockerfile               # Frontend container
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ app/
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ layout.tsx           # Main layout
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ page.tsx             # Home page
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ Chat.tsx             # Chat component
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ globals.css          # Global styles
â”‚   â”‚   â””â”€â”€ ğŸ“„ page.module.css      # Component styles
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“ public/
â”‚       â””â”€â”€ ğŸ“„ style.css            # Additional styles
```

---

## ğŸ”Œ API Documentation

### Available Endpoints

#### 1. POST /ask

Ask the chatbot a question

**Request**:

```json
{
  "question": "What is the conclusion of the document?"
}
```

**Response**:

```json
{
  "answer": "Based on the documents...",
  "sources": ["document.pdf"]
}
```

#### 2. GET /health

Check application status

**Response**:

```json
{
  "status": "ok",
  "vector_store_initialized": true
}
```

#### 3. GET /docs

Interactive Swagger UI Documentation

---

## ğŸ® Docker Commands

All commands use the `docker.sh` script:

```bash
# Build Docker images
./docker.sh build

# Start services in background
./docker.sh up

# Stop services
./docker.sh down

# Ingest documents
./docker.sh ingest

# Display logs in real-time
./docker.sh logs

# General usage
./docker.sh {build|up|down|ingest|logs}
```

### What Each Command Does

**`./docker.sh build`**

- Builds the backend Python container (Python 3.11-slim)
- Builds the frontend Node.js container (Node.js 20.11.1)
- Downloads and installs all dependencies
- Time: 3-5 minutes (first time)

**`./docker.sh up`**

- Starts all containers in the background
- Services are ready for use after a few seconds
- Shows output in the terminal

**`./docker.sh down`**

- Stops all running containers
- Removes container networks
- Data in vector_store is preserved

**`./docker.sh ingest`**

- Runs the document ingestion pipeline
- Loads documents from `backend/data/docs/`
- Splits into chunks (800 chars, 200 overlap)
- Generates embeddings using sentence-transformers
- Stores in Chroma vector database
- Time: 30-90 seconds

**`./docker.sh logs`**

- Shows real-time logs from all services
- Press Ctrl+C to stop
- Useful for debugging

---

## ğŸ”§ Configuration

### Change GROQ API Key

Edit `docker-compose.yml`:

```yaml
services:
  backend:
    environment:
      - GROQ_API_KEY=your_new_api_key_here
```

### Customize Ports

Edit `docker-compose.yml`:

```yaml
services:
  backend:
    ports:
      - "8080:8000" # Backend on port 8080

  frontend:
    ports:
      - "3001:3000" # Frontend on port 3001
```

### Adjust Document Chunking

Edit `backend/rag_pipeline.py`:

```python
def split_documents(self, docs):
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,      # Size of chunks in characters
        chunk_overlap=100    # Overlap between chunks
    )
    return splitter.split_documents(docs)
```

### Change Embedding Model

Edit `backend/rag_pipeline.py`:

```python
self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
# Or use other models: "all-mpnet-base-v2", "all-roberta-large-v1"
```

---

## ğŸ§ª Testing

### Quick Test

```bash
# 1. Build images
./docker.sh build

# 2. Start services
./docker.sh up

# 3. Wait a few seconds
sleep 10

# 4. Ingest documents
./docker.sh ingest

# 5. View logs
./docker.sh logs
```

### Test with curl

```bash
# Health check
curl http://localhost:8000/health

# Ask a question
curl -X POST http://localhost:8000/ask \
  -H "Content-Type: application/json" \
  -d '{"question": "What does this document explain?"}'
```

### Manual Testing

1. Open http://localhost:3000
2. Type a question
3. Check that the response appears
4. Verify source documents are shown

---

## ğŸ› Troubleshooting

### âŒ Error: "Docker is not running"

```bash
# Check Docker installation
docker --version

# Start Docker Desktop (macOS/Windows)
# Or start Docker daemon (Linux)
```

### âŒ Error: "Port 3000/8000 already in use"

```bash
# Option 1: Stop the other service
# Find what's using the port and stop it

# Option 2: Change ports in docker-compose.yml
# Edit the ports section for backend/frontend
```

### âŒ Error: "Vector store not initialized"

```bash
# Run ingestion
./docker.sh ingest

# Verify documents exist
ls -la ./backend/data/docs/
```

### âŒ First query is very slow

This is normal! The first query downloads the LLM model (~1GB).
Subsequent queries will be faster (2-5 seconds).

### âŒ No documents were ingested

```bash
# Check if documents are in the right folder
ls -la ./backend/data/docs/

# Verify supported formats (PDF, TXT, MD)

# Try ingestion again
./docker.sh ingest

# View logs
./docker.sh logs
```

### âŒ Answers are not relevant

- Check if documents are loaded correctly
- Documents must be readable text (not scanned images)
- Try with more specific questions

---

## ğŸ“Š Performance

| Metric                 | Duration/Size                  |
| ---------------------- | ------------------------------ |
| **Ingestion**          | 30-90 sec per 100 pages        |
| **First query**        | 15-30 sec (model download)     |
| **Subsequent queries** | 2-5 seconds                    |
| **RAM Memory**         | ~2GB backend + ~500MB frontend |
| **Vector storage**     | ~10MB per 1000 chunks          |

---

## ğŸ“‹ Technologies

### Backend

- **FastAPI** - Asynchronous web framework
- **LangChain** - RAG framework
- **Chroma** - Vector database
- **Sentence-Transformers** - Embedding model
- **GROQ API** - Cloud LLM service
- **Python 3.11** - Language

### Frontend

- **Next.js 16** - React SSR framework
- **React 19** - UI library
- **TypeScript 5** - Static typing
- **Tailwind CSS 4** - CSS framework
- **Node.js 20** - JavaScript runtime

### Infrastructure

- **Docker** - Containerization
- **Docker Compose** - Multi-container orchestration

---

## ğŸ” Security

### âš ï¸ Important Notes

- **No authentication** (development environment)
- **CORS enabled** for localhost only
- **API key in docker-compose.yml** - Keep it secret!

### ğŸ›¡ï¸ For Production

1. Add authentication (JWT, OAuth2)
2. Restrict CORS to authorized domains
3. Use secrets management (Vault, AWS Secrets Manager)
4. Enable HTTPS/TLS
5. Add rate limiting
6. Validate user input
7. Implement secure logging

---

## ğŸš€ Future Improvements

- [ ] User authentication
- [ ] Conversation history
- [ ] Support for DOCX, PPTX files
- [ ] Semantic caching
- [ ] Streaming responses
- [ ] Multi-language support
- [ ] Web search integration
- [ ] Document management UI

---

## ğŸ‘¤ Author

- **Project**: Quorium RAG Chatbot
- **Branch**: challenge/sabbar-rag-chatbot
- **Date**: December 2025

---

## ğŸ“„ License

This project was created for the Quorium AI Engineer Trainee coding challenge.

---

## ğŸ“ Support

For issues:

1. Check the **Troubleshooting** section
2. View logs: `./docker.sh logs`
3. Read API documentation: http://localhost:8000/docs

**Good luck! ğŸ‰**
